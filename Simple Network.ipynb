{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/endre/git/finance_ml/src\n",
      "Python: 3.6.1, TensorFlow:1.2.1, Keras:2.0.5\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import os\n",
    "import platform\n",
    "import tensorflow as tf\n",
    "import keras;\n",
    "\n",
    "# *print (\"Current file path: {}\".format(os.path.dirname(os.path.realpath(__file__))))\n",
    "print (\"Current Working Directory: {}\".format(os.getcwd()))\n",
    "print(\"Python: {}, TensorFlow:{}, Keras:{}\".\\\n",
    "      format(platform.python_version(), tf.__version__, keras.__version__))\n",
    "\n",
    "random_seed = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pickled Features, Labels and Name of each Feature-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De-pickling features, labels and feature/label names..\n",
      "De-pickle took 25464.75458200439 ms\n",
      "\n",
      "RangeNames/features/labels len: 1008922/1008922/1008922\n",
      "\n",
      "Number of Features for each feature-array: 54\n",
      "Number of Labels for each label-array: 4\n"
     ]
    }
   ],
   "source": [
    "print (\"De-pickling features, labels and feature/label names..\")\n",
    "load_start = timer()\n",
    "\n",
    "# pickled = pickle.load(open(\"RangeNamesFeaturesAndLabels-mk1.pickle\", \"rb\"))\n",
    "# pickled = pickle.load(open(\"RangeNamesFeaturesAndLabels-smallSet-6stocks.pickle\", \"rb\"))\n",
    "# pickled = pickle.load(open(\"RangeNamesFeaturesAndLabels-smallSet-50stocks.pickle\", \"rb\"))\n",
    "pickled = pickle.load(open(\"RangeNamesFeaturesAndLabels-mediumset-unknownNumStocks.pickle\", \"rb\"))\n",
    "\n",
    "load_millis = (timer()-load_start) * 1000\n",
    "print (\"De-pickle took {} ms\".format(load_millis))\n",
    "\n",
    "# {'rangeNames': rangeNames, 'features': features, 'labels': labels}\n",
    "rangeNames = pickled['rangeNames']\n",
    "features = pickled['features']\n",
    "labels = pickled['labels']\n",
    "print(\"\\nRangeNames/features/labels len: {}/{}/{}\".\\\n",
    "      format(len(rangeNames), len(features), len(labels)))\n",
    "numFeatures = len(features[0])\n",
    "print(\"\\nNumber of Features for each feature-array: {}\".format(numFeatures))\n",
    "print(\"Number of Labels for each label-array: {}\".format(len(labels[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split set into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 7899.90 MB\n",
      "Memory usage: 7899.90 MB\n",
      "Memory usage: 7899.90 MB\n",
      "TRAIN RangeNames/features/labels len: 939830/939830/939830 - 93.15% of total\n",
      "TEST RangeNames/features/labels len: 69092/69092/69092 - 6.85% of total\n",
      "\n",
      "Total loaded features: 1008922, trainFeatures + testFeatures:1008922 - 100.0000%\n"
     ]
    }
   ],
   "source": [
    "import resource\n",
    "import gc\n",
    "def mem():\n",
    "    print('Memory usage: %2.2f MB' % round(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024.0,1))\n",
    "    \n",
    "mem()\n",
    "trainRangeNames = []\n",
    "trainFeatures = []\n",
    "trainLabels = []\n",
    "\n",
    "testRangeNames = []\n",
    "testFeatures = []\n",
    "testLabels = []\n",
    "mem()\n",
    "\n",
    "gc.collect()\n",
    "mem()\n",
    "\n",
    "length = len(rangeNames)\n",
    "for i in range(length):\n",
    "    names = rangeNames[i]\n",
    "    if (names[1] < '2016-01-01'):\n",
    "        trainRangeNames.append(names)\n",
    "        trainFeatures.append(features[i])\n",
    "        trainLabels.append(labels[i])\n",
    "    else:\n",
    "        testRangeNames.append(names)\n",
    "        testFeatures.append(features[i])\n",
    "        testLabels.append(labels[i])\n",
    "\n",
    "lenTrainFeatures = len(trainFeatures)\n",
    "\n",
    "print(\"TRAIN RangeNames/features/labels len: {}/{}/{} - {:.2f}% of total\".\\\n",
    "      format(len(trainRangeNames), lenTrainFeatures, len(trainLabels), (lenTrainFeatures / len(features)) * 100 ))\n",
    "print(\"TEST RangeNames/features/labels len: {}/{}/{} - {:.2f}% of total\".\\\n",
    "      format(len(testRangeNames), len(testFeatures), len(testLabels), (len(testFeatures) / len(features)) * 100 ))\n",
    "\n",
    "print(\"\\nTotal loaded features: {}, trainFeatures + testFeatures:{} - {:.4f}%\".\\\n",
    "      format(len(features), lenTrainFeatures + len(testFeatures), ((lenTrainFeatures + len(testFeatures)) / len(features)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hotted ok.\n",
      "Train labels [good, bad]: [505724, 434106] -> [53.81%, 46.19%]\n",
      "Test labels  [good, bad]: [39999, 29093] -> [57.89%, 42.11%]\n"
     ]
    }
   ],
   "source": [
    "# One-hot the labels\n",
    "\n",
    "# 0:  5 days\n",
    "# 1: 10 days\n",
    "# 2: 15 days\n",
    "# 3: 20 days\n",
    "label_to_use = 3\n",
    "\n",
    "good = [1,0]\n",
    "bad =  [0,1]\n",
    "\n",
    "trainLabels_onehot = [good if x[label_to_use] > 0 else bad  for x in trainLabels]\n",
    "testLabels_onehot = [good if x[label_to_use] > 0 else bad  for x in testLabels]\n",
    "print(\"One-hotted ok.\")\n",
    "\n",
    "sum_trainLabels = [sum(i) for i in zip(*trainLabels_onehot)]\n",
    "sum_testLabels = [sum(i) for i in zip(*testLabels_onehot)]\n",
    "print(\"Train labels [good, bad]: {} -> [{:.2f}%, {:.2f}%]\".\\\n",
    "      format(sum_trainLabels, (sum_trainLabels[0] / len(trainLabels)) * 100, (sum_trainLabels[1] / len(trainLabels)) * 100))\n",
    "print(\"Test labels  [good, bad]: {} -> [{:.2f}%, {:.2f}%]\".\\\n",
    "      format(sum_testLabels, (sum_testLabels[0] / len(testLabels)) * 100, (sum_testLabels[1] / len(testLabels)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !! RUN FROM HERE WHEN ONLY TWEAKING NETWORK !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "currentTrainPos = 0\n",
    "shuff_trainFeatures = []\n",
    "shuff_trainLabels_onehot = []\n",
    "def getTrainMiniBatch(miniBatchSize):\n",
    "    global currentTrainPos\n",
    "    global shuff_trainFeatures, shuff_trainLabels_onehot\n",
    "    if (currentTrainPos + miniBatchSize > lenTrainFeatures):\n",
    "        currentTrainPos = 0\n",
    "    if (currentTrainPos == 0):\n",
    "        print(\"Shuffling training features and labels..\")\n",
    "        shuff_trainFeatures, shuff_trainLabels_onehot = shuffle(trainFeatures, trainLabels_onehot, random_state=random_seed)\n",
    "    start = currentTrainPos\n",
    "    end = currentTrainPos + miniBatchSize\n",
    "    currentTrainPos = end\n",
    "    \n",
    "    return shuff_trainFeatures[start:end], shuff_trainLabels_onehot[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini batches per epoch: trainFeats 939830 / mini_batch_size 256 = 3671\n",
      "Total mini batches: mini_batches_per_epoch 3671 * epochs 5 = 18355\n",
      "Network set up.\n"
     ]
    }
   ],
   "source": [
    "n1=1200\n",
    "n2=1800\n",
    "n3=1200\n",
    "n4=600\n",
    "out=2\n",
    "epochs = 5 # Not explicitly used, as we count mini_batches\n",
    "mini_batch_size = 256\n",
    "learning_rate = 0.0001  # 0.0001 is good, but maybe use higher learning rate when dropout'ing..\n",
    "keep_prob_in_train = 0.8\n",
    "keep_prob_hid_train = 0.5\n",
    "transfer_function = tf.nn.elu\n",
    "l2_regularization_beta = 0.025\n",
    "\n",
    "#========\n",
    "mini_batches_per_epoch = int(lenTrainFeatures / mini_batch_size)\n",
    "total_mini_batches = mini_batches_per_epoch * epochs\n",
    "\n",
    "print(\"Mini batches per epoch: trainFeats {} / mini_batch_size {} = {}\".format(lenTrainFeatures, mini_batch_size, mini_batches_per_epoch))\n",
    "print(\"Total mini batches: mini_batches_per_epoch {} * epochs {} = {}\".format(mini_batches_per_epoch, epochs, total_mini_batches))\n",
    "\n",
    "# Reset the default graph, so as to chuck out existing variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"input\"):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, numFeatures], name=\"X\")\n",
    "    keep_prob_in = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    X_drop = tf.nn.dropout(X, keep_prob_in)\n",
    "    labels = tf.placeholder(tf.float32, shape=[None, out], name=\"labels\")\n",
    "\n",
    "with tf.variable_scope(\"hidden_layers_common\"):\n",
    "    keep_prob_hid = tf.placeholder(tf.float32)\n",
    "\n",
    "def weight_variable(name, shape):\n",
    "    # return tf.get_variable(name, initializer=tf.glorot_uniform_initializer(seed=random_seed), shape=shape)\n",
    "    return tf.get_variable(name, initializer=tf.contrib.layers.xavier_initializer(), shape=shape)\n",
    "    \n",
    "def bias_variable(name, shape):\n",
    "    # return tf.get_variable(name, initializer=tf.glorot_uniform_initializer(seed=random_seed), shape=shape)\n",
    "    return tf.get_variable(name, initializer=tf.contrib.layers.xavier_initializer(), shape=shape)\n",
    "\n",
    "with tf.variable_scope(\"layer1\"):\n",
    "    w1 = weight_variable(\"weight\", [numFeatures, n1])\n",
    "    b1 = bias_variable(\"bias\", [n1])\n",
    "    l1 = tf.matmul(X_drop, w1) + b1\n",
    "    l1 = transfer_function(l1)\n",
    "    l1 = tf.nn.dropout(l1, keep_prob_hid)\n",
    "\n",
    "with tf.variable_scope(\"layer2\"):\n",
    "    w2 = weight_variable(\"weight\", [n1, n2])\n",
    "    b2 = bias_variable(\"bias\", [n2])\n",
    "    l2 = tf.matmul(l1, w2) + b2\n",
    "    l2 = transfer_function(l2)\n",
    "    l2 = tf.nn.dropout(l2, keep_prob_hid)\n",
    "\n",
    "with tf.variable_scope(\"layer3\"):\n",
    "    w3 = weight_variable(\"weight\", [n2, n3])\n",
    "    b3 = bias_variable(\"bias\", [n3])\n",
    "    l3 = tf.matmul(l2, w3) + b3\n",
    "    l3 = transfer_function(l3)\n",
    "    l3 = tf.nn.dropout(l3, keep_prob_hid)\n",
    "\n",
    "with tf.variable_scope(\"layer4\"):\n",
    "    w4 = weight_variable(\"weight\", [n3, n4])\n",
    "    b4 = bias_variable(\"bias\", [n4])\n",
    "    l4 = tf.matmul(l3, w4) + b4\n",
    "    l4 = transfer_function(l4)\n",
    "    l4 = tf.nn.dropout(l4, keep_prob_hid)\n",
    "\n",
    "with tf.variable_scope(\"output\"):\n",
    "    wy = weight_variable(\"weight\", [n4, out])\n",
    "    by = bias_variable(\"bias\", [out])\n",
    "    Y = tf.matmul(l4, wy) + by\n",
    "\n",
    "# :: Loss function with L2 Regularization\n",
    "l2_regularizer = tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2) + tf.nn.l2_loss(w3) + \\\n",
    "                 tf.nn.l2_loss(w4) + tf.nn.l2_loss(wy)\n",
    "# NOTE: L1 regularizer: tf.reduce_sum(tf.abs(tensor)) instead of tf.nn.l2_loss(tensor)\n",
    "# Read here: https://github.com/tensorflow/models/blob/master/inception/inception/slim/losses.py\n",
    "# Max-norm:\n",
    "# https://stackoverflow.com/questions/37801832/how-can-i-implement-max-norm-constraints-in-an-mlp-in-tensorflow\n",
    "# Hmm..: https://stackoverflow.com/questions/34934303/renormalize-weight-matrix-using-tensorflow\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y, labels=labels) +\\\n",
    "                      l2_regularization_beta * l2_regularizer)\n",
    "\n",
    "# :: Optimizer/Trainer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# Define Test/Evaluate: Accuracy: Fraction right predictions\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"Network set up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE Training\n",
      "  TRAIN cost/acc:49.3260/47.1791%, TEST cost/acc:49.3647/44.4465%\n",
      "-----------------------------------\n",
      "\n",
      "Shuffling training features and labels..\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 100 of 18355\n",
      "  TRAIN cost/acc:46.7166/53.7313%, TEST cost/acc:46.6467/56.8836%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 200 of 18355\n",
      "  TRAIN cost/acc:44.9110/53.9599%, TEST cost/acc:44.8659/55.8849%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 300 of 18355\n",
      "  TRAIN cost/acc:43.2519/53.8485%, TEST cost/acc:43.2087/57.1716%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 400 of 18355\n",
      "  TRAIN cost/acc:41.4962/53.8123%, TEST cost/acc:41.4658/56.9603%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 500 of 18355\n",
      "  TRAIN cost/acc:39.7761/53.9223%, TEST cost/acc:39.7386/57.6709%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 600 of 18355\n",
      "  TRAIN cost/acc:37.9926/53.5822%, TEST cost/acc:37.9717/54.3623%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 700 of 18355\n",
      "  TRAIN cost/acc:36.2387/53.9527%, TEST cost/acc:36.2196/57.3872%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 800 of 18355\n",
      "  TRAIN cost/acc:34.5019/53.9281%, TEST cost/acc:34.4808/57.1991%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 900 of 18355\n",
      "  TRAIN cost/acc:32.7767/53.9484%, TEST cost/acc:32.7664/56.6751%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 1000 of 18355\n",
      "  TRAIN cost/acc:31.0822/53.9295%, TEST cost/acc:31.0725/57.1282%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 1100 of 18355\n",
      "  TRAIN cost/acc:29.4154/53.9194%, TEST cost/acc:29.4107/55.4811%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 1200 of 18355\n",
      "  TRAIN cost/acc:27.7893/53.9281%, TEST cost/acc:27.7820/56.5434%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 1300 of 18355\n",
      "  TRAIN cost/acc:26.2031/54.0381%, TEST cost/acc:26.1965/56.6201%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 1400 of 18355\n",
      "  TRAIN cost/acc:24.6658/53.9078%, TEST cost/acc:24.6581/56.8633%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 1500 of 18355\n",
      "  TRAIN cost/acc:23.1799/54.0106%, TEST cost/acc:23.1712/57.2353%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 1600 of 18355\n",
      "  TRAIN cost/acc:21.7484/53.8905%, TEST cost/acc:21.7391/56.7533%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 1700 of 18355\n",
      "  TRAIN cost/acc:20.3690/53.9237%, TEST cost/acc:20.3625/56.5073%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 1800 of 18355\n",
      "  TRAIN cost/acc:19.0526/53.8340%, TEST cost/acc:19.0475/56.0224%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 1900 of 18355\n",
      "  TRAIN cost/acc:17.7981/53.8383%, TEST cost/acc:17.7981/53.1726%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 2000 of 18355\n",
      "  TRAIN cost/acc:16.6096/53.7544%, TEST cost/acc:16.6048/56.0441%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 2100 of 18355\n",
      "  TRAIN cost/acc:15.4745/53.7834%, TEST cost/acc:15.4724/54.8486%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 2200 of 18355\n",
      "  TRAIN cost/acc:14.4194/53.7341%, TEST cost/acc:14.4164/55.3798%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 2300 of 18355\n",
      "  TRAIN cost/acc:13.4132/53.6343%, TEST cost/acc:13.4118/53.4693%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 2400 of 18355\n",
      "  TRAIN cost/acc:12.4731/53.6676%, TEST cost/acc:12.4688/55.9052%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 2500 of 18355\n",
      "  TRAIN cost/acc:11.5863/53.6516%, TEST cost/acc:11.5822/54.3999%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 2600 of 18355\n",
      "  TRAIN cost/acc:10.7535/53.6430%, TEST cost/acc:10.7503/53.2160%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 2700 of 18355\n",
      "  TRAIN cost/acc:9.9710/53.6314%, TEST cost/acc:9.9666/54.0772%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 2800 of 18355\n",
      "  TRAIN cost/acc:9.2413/53.5301%, TEST cost/acc:9.2363/53.5344%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 2900 of 18355\n",
      "  TRAIN cost/acc:8.5589/53.6473%, TEST cost/acc:8.5592/51.8092%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 3000 of 18355\n",
      "  TRAIN cost/acc:7.9198/53.8745%, TEST cost/acc:7.9184/53.1870%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 3100 of 18355\n",
      "  TRAIN cost/acc:7.3290/54.0424%, TEST cost/acc:7.3248/53.4649%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 3200 of 18355\n",
      "  TRAIN cost/acc:6.7751/54.0366%, TEST cost/acc:6.7720/55.5752%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 3300 of 18355\n",
      "  TRAIN cost/acc:6.2625/54.0453%, TEST cost/acc:6.2578/54.9499%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 3400 of 18355\n",
      "  TRAIN cost/acc:5.7876/54.0815%, TEST cost/acc:5.7803/55.9631%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 3500 of 18355\n",
      "  TRAIN cost/acc:5.3436/54.0960%, TEST cost/acc:5.3386/56.6201%\n",
      " ... progress: In \"Epoch\" 0 of 5, mini batch 3600 of 18355\n",
      "  TRAIN cost/acc:4.9400/54.0815%, TEST cost/acc:4.9333/56.9168%\n",
      "\n",
      "After \"Epoch\" 1 of 5, mini batch 3671 of 18355\n",
      "  TRAIN cost/acc:4.6731/54.0251%, TEST cost/acc:4.6627/57.5855%\n",
      "Shuffling training features and labels..\n",
      " ... progress: In \"Epoch\" 1 of 5, mini batch 4000 of 18355\n",
      " ... progress: In \"Epoch\" 1 of 5, mini batch 4500 of 18355\n",
      " ... progress: In \"Epoch\" 1 of 5, mini batch 5000 of 18355\n",
      " ... progress: In \"Epoch\" 1 of 5, mini batch 5500 of 18355\n",
      " ... progress: In \"Epoch\" 1 of 5, mini batch 6000 of 18355\n",
      " ... progress: In \"Epoch\" 1 of 5, mini batch 6500 of 18355\n",
      " ... progress: In \"Epoch\" 1 of 5, mini batch 7000 of 18355\n",
      "\n",
      "After \"Epoch\" 2 of 5, mini batch 7342 of 18355\n",
      "  TRAIN cost/acc:0.7554/53.9455%, TEST cost/acc:0.7486/57.8924%\n",
      "Shuffling training features and labels..\n",
      " ... progress: In \"Epoch\" 2 of 5, mini batch 7500 of 18355\n",
      " ... progress: In \"Epoch\" 2 of 5, mini batch 8000 of 18355\n",
      " ... progress: In \"Epoch\" 2 of 5, mini batch 8500 of 18355\n",
      " ... progress: In \"Epoch\" 2 of 5, mini batch 9000 of 18355\n",
      " ... progress: In \"Epoch\" 2 of 5, mini batch 9500 of 18355\n",
      " ... progress: In \"Epoch\" 2 of 5, mini batch 10000 of 18355\n",
      " ... progress: In \"Epoch\" 2 of 5, mini batch 10500 of 18355\n",
      " ... progress: In \"Epoch\" 2 of 5, mini batch 11000 of 18355\n",
      "\n",
      "After \"Epoch\" 3 of 5, mini batch 11013 of 18355\n",
      "  TRAIN cost/acc:0.6926/53.9440%, TEST cost/acc:0.6849/57.8924%\n",
      "Shuffling training features and labels..\n",
      " ... progress: In \"Epoch\" 3 of 5, mini batch 11500 of 18355\n",
      " ... progress: In \"Epoch\" 3 of 5, mini batch 12000 of 18355\n",
      " ... progress: In \"Epoch\" 3 of 5, mini batch 12500 of 18355\n",
      " ... progress: In \"Epoch\" 3 of 5, mini batch 13000 of 18355\n",
      " ... progress: In \"Epoch\" 3 of 5, mini batch 13500 of 18355\n",
      " ... progress: In \"Epoch\" 3 of 5, mini batch 14000 of 18355\n",
      " ... progress: In \"Epoch\" 3 of 5, mini batch 14500 of 18355\n",
      "\n",
      "After \"Epoch\" 4 of 5, mini batch 14684 of 18355\n",
      "  TRAIN cost/acc:0.6909/53.9440%, TEST cost/acc:0.6833/57.8924%\n",
      "Shuffling training features and labels..\n",
      " ... progress: In \"Epoch\" 4 of 5, mini batch 15000 of 18355\n",
      " ... progress: In \"Epoch\" 4 of 5, mini batch 15500 of 18355\n",
      " ... progress: In \"Epoch\" 4 of 5, mini batch 16000 of 18355\n",
      " ... progress: In \"Epoch\" 4 of 5, mini batch 16500 of 18355\n",
      " ... progress: In \"Epoch\" 4 of 5, mini batch 17000 of 18355\n",
      " ... progress: In \"Epoch\" 4 of 5, mini batch 17500 of 18355\n",
      " ... progress: In \"Epoch\" 4 of 5, mini batch 18000 of 18355\n",
      "\n",
      "After \"Epoch\" 5 of 5, mini batch 18355 of 18355\n",
      "  TRAIN cost/acc:0.6907/53.9440%, TEST cost/acc:0.6832/57.8924%\n",
      "\n",
      "FINISHED\n",
      "  TRAIN cost/acc:0.6907/53.9440%, TEST cost/acc:0.6832/57.8924%\n",
      "TRAINING took 154.57439519399486 seconds.\n"
     ]
    }
   ],
   "source": [
    "check_length = len(testFeatures)\n",
    "check_trainFeatures, check_trainLabels_onehot = shuffle(trainFeatures, trainLabels_onehot, random_state=42)\n",
    "check_trainFeatures = check_trainFeatures[:check_length]\n",
    "check_trainLabels_onehot = check_trainLabels_onehot[:check_length]\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialize TensorFlow variables in Session\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def printStats(what):\n",
    "    train_cost, train_accuracy = sess.run([loss, accuracy], \n",
    "                                       feed_dict={X: check_trainFeatures,\n",
    "                                                  labels: check_trainLabels_onehot,\n",
    "                                                  keep_prob_in: 1.0,\n",
    "                                                  keep_prob_hid: 1.0})\n",
    "    test_cost, test_accuracy = sess.run([loss, accuracy], \n",
    "                                     feed_dict={X: testFeatures,\n",
    "                                                labels: testLabels_onehot,\n",
    "                                                keep_prob_in: 1.0,\n",
    "                                                keep_prob_hid: 1.0})\n",
    "    print(\"{}\\n  TRAIN cost/acc:{:.4f}/{:.4f}%, TEST cost/acc:{:.4f}/{:.4f}%\"\\\n",
    "          .format(what, train_cost, train_accuracy*100, test_cost, test_accuracy*100))\n",
    "\n",
    "printStats(\"BEFORE Training\")\n",
    "print(\"-----------------------------------\\n\")\n",
    "\n",
    "time_start = timer()\n",
    "epoch = 0\n",
    "for mini_batch in range(1, total_mini_batches+1):\n",
    "    batch_inputs, batch_labels = getTrainMiniBatch(mini_batch_size)\n",
    "    sess.run([train, accuracy], feed_dict={X: batch_inputs, \n",
    "                                           labels: batch_labels,\n",
    "                                           keep_prob_in: keep_prob_in_train,\n",
    "                                           keep_prob_hid: keep_prob_hid_train})\n",
    "    if ((epoch == 0) and (mini_batch % 100 == 0)):\n",
    "        printStats(' ... progress: In \"Epoch\" {} of {}, mini batch {} of {}'.format(epoch, epochs, mini_batch, total_mini_batches))\n",
    "    if ((epoch > 0) and (mini_batch % 500 == 0)):\n",
    "        print(' ... progress: In \"Epoch\" {} of {}, mini batch {} of {}'.format(epoch, epochs, mini_batch, total_mini_batches))\n",
    "        \n",
    "    if (mini_batch % mini_batches_per_epoch == 0):\n",
    "        epoch += 1\n",
    "        printStats('\\nAfter \"Epoch\" {} of {}, mini batch {} of {}'.format(epoch, epochs, mini_batch, total_mini_batches))\n",
    "        \n",
    "training_time = timer()-time_start\n",
    "printStats(\"\\nFINISHED\")\n",
    "print(\"TRAINING took {} seconds.\".format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:endre-tf]",
   "language": "python",
   "name": "conda-env-endre-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
