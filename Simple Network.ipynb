{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/endre/git/finance_ml/src\n",
      "Python: 3.6.1, TensorFlow:1.2.1, Keras:2.0.5\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import os\n",
    "import platform\n",
    "import tensorflow as tf\n",
    "import keras;\n",
    "\n",
    "# *print (\"Current file path: {}\".format(os.path.dirname(os.path.realpath(__file__))))\n",
    "print (\"Current Working Directory: {}\".format(os.getcwd()))\n",
    "print(\"Python: {}, TensorFlow:{}, Keras:{}\".\\\n",
    "      format(platform.python_version(), tf.__version__, keras.__version__))\n",
    "\n",
    "random_seed = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pickled Features, Labels and Name of each Feature-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De-pickling features, labels and feature/label names..\n",
      "De-pickle took 25738.598705996992 ms\n",
      "\n",
      "RangeNames/features/labels len: 1008922/1008922/1008922\n",
      "\n",
      "Number of Features for each feature-array: 54\n",
      "Number of Labels for each label-array: 4\n"
     ]
    }
   ],
   "source": [
    "print (\"De-pickling features, labels and feature/label names..\")\n",
    "load_start = timer()\n",
    "\n",
    "# pickled = pickle.load(open(\"RangeNamesFeaturesAndLabels-mk1.pickle\", \"rb\"))\n",
    "# pickled = pickle.load(open(\"RangeNamesFeaturesAndLabels-smallSet-6stocks.pickle\", \"rb\"))\n",
    "# pickled = pickle.load(open(\"RangeNamesFeaturesAndLabels-smallSet-50stocks.pickle\", \"rb\"))\n",
    "pickled = pickle.load(open(\"RangeNamesFeaturesAndLabels-mediumset-unknownNumStocks.pickle\", \"rb\"))\n",
    "\n",
    "load_millis = (timer()-load_start) * 1000\n",
    "print (\"De-pickle took {} ms\".format(load_millis))\n",
    "\n",
    "# {'rangeNames': rangeNames, 'features': features, 'labels': labels}\n",
    "rangeNames = pickled['rangeNames']\n",
    "features = pickled['features']\n",
    "labels = pickled['labels']\n",
    "print(\"\\nRangeNames/features/labels len: {}/{}/{}\".\\\n",
    "      format(len(rangeNames), len(features), len(labels)))\n",
    "numFeatures = len(features[0])\n",
    "print(\"\\nNumber of Features for each feature-array: {}\".format(numFeatures))\n",
    "print(\"Number of Labels for each label-array: {}\".format(len(labels[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split set into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 7895.80 MB\n",
      "Memory usage: 7895.80 MB\n",
      "Memory usage: 7895.80 MB\n",
      "TRAIN RangeNames/features/labels len: 939830/939830/939830 - 93.15% of total\n",
      "TEST RangeNames/features/labels len: 69092/69092/69092 - 6.85% of total\n",
      "\n",
      "Total loaded features: 1008922, trainFeatures + testFeatures:1008922 - 100.0000%\n"
     ]
    }
   ],
   "source": [
    "import resource\n",
    "import gc\n",
    "def mem():\n",
    "    print('Memory usage: %2.2f MB' % round(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024.0,1))\n",
    "    \n",
    "mem()\n",
    "trainRangeNames = []\n",
    "trainFeatures = []\n",
    "trainLabels = []\n",
    "\n",
    "testRangeNames = []\n",
    "testFeatures = []\n",
    "testLabels = []\n",
    "mem()\n",
    "\n",
    "gc.collect()\n",
    "mem()\n",
    "\n",
    "length = len(rangeNames)\n",
    "for i in range(length):\n",
    "    names = rangeNames[i]\n",
    "    if (names[1] < '2016-01-01'):\n",
    "        trainRangeNames.append(names)\n",
    "        trainFeatures.append(features[i])\n",
    "        trainLabels.append(labels[i])\n",
    "    else:\n",
    "        testRangeNames.append(names)\n",
    "        testFeatures.append(features[i])\n",
    "        testLabels.append(labels[i])\n",
    "\n",
    "lenTrainFeatures = len(trainFeatures)\n",
    "\n",
    "print(\"TRAIN RangeNames/features/labels len: {}/{}/{} - {:.2f}% of total\".\\\n",
    "      format(len(trainRangeNames), lenTrainFeatures, len(trainLabels), (lenTrainFeatures / len(features)) * 100 ))\n",
    "print(\"TEST RangeNames/features/labels len: {}/{}/{} - {:.2f}% of total\".\\\n",
    "      format(len(testRangeNames), len(testFeatures), len(testLabels), (len(testFeatures) / len(features)) * 100 ))\n",
    "\n",
    "print(\"\\nTotal loaded features: {}, trainFeatures + testFeatures:{} - {:.4f}%\".\\\n",
    "      format(len(features), lenTrainFeatures + len(testFeatures), ((lenTrainFeatures + len(testFeatures)) / len(features)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hotted ok.\n",
      "Train labels [good, bad]: [505724, 434106] -> [53.81%, 46.19%]\n",
      "Test labels  [good, bad]: [39999, 29093] -> [57.89%, 42.11%]\n"
     ]
    }
   ],
   "source": [
    "# One-hot the labels\n",
    "\n",
    "# 0:  5 days\n",
    "# 1: 10 days\n",
    "# 2: 15 days\n",
    "# 3: 20 days\n",
    "label_to_use = 3\n",
    "\n",
    "good = [1,0]\n",
    "bad =  [0,1]\n",
    "\n",
    "oh_trainLabels = [good if x[label_to_use] > 0 else bad  for x in trainLabels]\n",
    "oh_testLabels = [good if x[label_to_use] > 0 else bad  for x in testLabels]\n",
    "print(\"One-hotted ok.\")\n",
    "\n",
    "sum_trainLabels = [sum(i) for i in zip(*oh_trainLabels)]\n",
    "sum_testLabels = [sum(i) for i in zip(*oh_testLabels)]\n",
    "print(\"Train labels [good, bad]: {} -> [{:.2f}%, {:.2f}%]\".\\\n",
    "      format(sum_trainLabels, (sum_trainLabels[0] / len(trainLabels)) * 100, (sum_trainLabels[1] / len(trainLabels)) * 100))\n",
    "print(\"Test labels  [good, bad]: {} -> [{:.2f}%, {:.2f}%]\".\\\n",
    "      format(sum_testLabels, (sum_testLabels[0] / len(testLabels)) * 100, (sum_testLabels[1] / len(testLabels)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "currentTrainPos = 0\n",
    "def getTrainMiniBatch(miniBatchSize):\n",
    "    global currentTrainPos\n",
    "    global trainRangeNames, trainFeatures, trainLabels, oh_trainLabels\n",
    "    if (currentTrainPos + miniBatchSize > lenTrainFeatures):\n",
    "        currentTrainPos = 0\n",
    "    if (currentTrainPos == 0):\n",
    "        trainRangeNames, trainFeatures, trainLabels, oh_trainLabels =\\\n",
    "            shuffle(trainRangeNames, trainFeatures, trainLabels, oh_trainLabels, random_state=random_seed)\n",
    "    start = currentTrainPos\n",
    "    end = currentTrainPos + miniBatchSize\n",
    "    currentTrainPos = end\n",
    "    \n",
    "    return trainFeatures[start:end], oh_trainLabels[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini batches per epoch: trainFeats 939830 / mini_batch_size 1024 = 917\n",
      "Total mini batches: mini_batches_per_epoch 917 * epochs 15 = 13755\n",
      "Network set up.\n"
     ]
    }
   ],
   "source": [
    "n1=800\n",
    "n2=1500\n",
    "n3=800\n",
    "out=2\n",
    "epochs = 15   # Not explicitly used, as we count mini_batches\n",
    "mini_batch_size = 1024\n",
    "learning_rate = 0.0001\n",
    "transfer_function = tf.nn.relu\n",
    "\n",
    "#========\n",
    "mini_batches_per_epoch = int(lenTrainFeatures / mini_batch_size)\n",
    "total_mini_batches = mini_batches_per_epoch * epochs\n",
    "\n",
    "print(\"Mini batches per epoch: trainFeats {} / mini_batch_size {} = {}\".format(lenTrainFeatures, mini_batch_size, mini_batches_per_epoch))\n",
    "print(\"Total mini batches: mini_batches_per_epoch {} * epochs {} = {}\".format(mini_batches_per_epoch, epochs, total_mini_batches))\n",
    "\n",
    "# Reset the default graph, so as to chuck out existing variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, numFeatures])\n",
    "labels = tf.placeholder(tf.float32, shape=[None, out])\n",
    "\n",
    "def weight_variable(name, shape):\n",
    "    # return tf.get_variable(name, initializer=tf.glorot_uniform_initializer(seed=random_seed), shape=shape)\n",
    "    return tf.get_variable(name, initializer=tf.contrib.layers.xavier_initializer(), shape=shape)\n",
    "    \n",
    "def bias_variable(name, shape):\n",
    "    # return tf.get_variable(name, initializer=tf.glorot_uniform_initializer(seed=random_seed), shape=shape)\n",
    "    return tf.get_variable(name, initializer=tf.contrib.layers.xavier_initializer(), shape=shape)\n",
    "\n",
    "with tf.variable_scope(\"layer1\"):\n",
    "    w1 = weight_variable(\"weight\", [numFeatures, n1])\n",
    "    b1 = bias_variable(\"bias\", [n1])\n",
    "    l1 = tf.matmul(X, w1) + b1\n",
    "    l1 = transfer_function(l1)\n",
    "\n",
    "with tf.variable_scope(\"layer2\"):\n",
    "    w2 = weight_variable(\"weight\", [n1, n2])\n",
    "    b2 = bias_variable(\"bias\", [n2])\n",
    "    l2 = tf.matmul(l1, w2) + b2\n",
    "    l2 = transfer_function(l2)\n",
    "\n",
    "with tf.variable_scope(\"layer3\"):\n",
    "    w3 = weight_variable(\"weight\", [n2, n3])\n",
    "    b3 = bias_variable(\"bias\", [n3])\n",
    "    l3 = tf.matmul(l2, w3) + b3\n",
    "    l3 = transfer_function(l3)\n",
    "\n",
    "with tf.variable_scope(\"output\"):\n",
    "    wy = weight_variable(\"weight\", [n3, out])\n",
    "    by = bias_variable(\"bias\", [out])\n",
    "    Y = tf.matmul(l3, wy) + by\n",
    "\n",
    "# Define Training: Loss/Cost and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Define Test/Evaluate: Accuracy: Fraction right predictions\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"Network set up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE Training\n",
      "  TRAIN cost/acc:1.6980/53.5200%, TEST cost/acc:1.4199/57.1875%\n",
      "\n",
      "After \"Epoch\" 1, mini batch 917 of 13755\n",
      "  TRAIN cost/acc:0.7236/53.8100%, TEST cost/acc:0.7024/55.3769%\n",
      " ... progress: In \"Epoch\" 1, mini batch 1000 of 13755\n",
      "  TRAIN cost/acc:0.7058/55.5600%, TEST cost/acc:0.7130/53.2898%\n",
      "\n",
      "After \"Epoch\" 2, mini batch 1834 of 13755\n",
      "  TRAIN cost/acc:0.6896/57.0100%, TEST cost/acc:0.7082/52.8976%\n",
      " ... progress: In \"Epoch\" 2, mini batch 2000 of 13755\n",
      "  TRAIN cost/acc:0.7503/56.4600%, TEST cost/acc:0.7079/55.1395%\n",
      "\n",
      "After \"Epoch\" 3, mini batch 2751 of 13755\n",
      "  TRAIN cost/acc:0.7029/55.1500%, TEST cost/acc:0.7068/52.5271%\n",
      " ... progress: In \"Epoch\" 3, mini batch 3000 of 13755\n",
      "  TRAIN cost/acc:0.6846/56.9000%, TEST cost/acc:0.6997/53.7182%\n",
      "\n",
      "After \"Epoch\" 4, mini batch 3668 of 13755\n",
      "  TRAIN cost/acc:0.6775/56.9300%, TEST cost/acc:0.6956/55.2075%\n",
      " ... progress: In \"Epoch\" 4, mini batch 4000 of 13755\n",
      "  TRAIN cost/acc:0.6735/58.3300%, TEST cost/acc:0.6976/54.1394%\n",
      "\n",
      "After \"Epoch\" 5, mini batch 4585 of 13755\n",
      "  TRAIN cost/acc:0.6758/58.1400%, TEST cost/acc:0.6985/54.6127%\n",
      " ... progress: In \"Epoch\" 5, mini batch 5000 of 13755\n",
      "  TRAIN cost/acc:0.6715/58.1000%, TEST cost/acc:0.7024/53.3897%\n",
      "\n",
      "After \"Epoch\" 6, mini batch 5502 of 13755\n",
      "  TRAIN cost/acc:0.6743/57.7100%, TEST cost/acc:0.7002/54.0497%\n",
      " ... progress: In \"Epoch\" 6, mini batch 6000 of 13755\n",
      "  TRAIN cost/acc:0.6823/58.4200%, TEST cost/acc:0.7051/53.7066%\n",
      "\n",
      "After \"Epoch\" 7, mini batch 6419 of 13755\n",
      "  TRAIN cost/acc:0.6805/57.7100%, TEST cost/acc:0.7020/53.9889%\n",
      " ... progress: In \"Epoch\" 7, mini batch 7000 of 13755\n",
      "  TRAIN cost/acc:0.6688/58.9100%, TEST cost/acc:0.7038/54.5360%\n",
      "\n",
      "After \"Epoch\" 8, mini batch 7336 of 13755\n",
      "  TRAIN cost/acc:0.6833/58.6300%, TEST cost/acc:0.7056/53.0090%\n",
      " ... progress: In \"Epoch\" 8, mini batch 8000 of 13755\n",
      "  TRAIN cost/acc:0.6645/58.7000%, TEST cost/acc:0.7027/54.7068%\n",
      "\n",
      "After \"Epoch\" 9, mini batch 8253 of 13755\n",
      "  TRAIN cost/acc:0.6627/58.9200%, TEST cost/acc:0.7079/53.1494%\n",
      " ... progress: In \"Epoch\" 9, mini batch 9000 of 13755\n",
      "  TRAIN cost/acc:0.6621/60.0200%, TEST cost/acc:0.7127/52.7977%\n",
      "\n",
      "After \"Epoch\" 10, mini batch 9170 of 13755\n",
      "  TRAIN cost/acc:0.6600/60.2300%, TEST cost/acc:0.7108/53.1639%\n",
      " ... progress: In \"Epoch\" 10, mini batch 10000 of 13755\n",
      "  TRAIN cost/acc:0.6508/60.5500%, TEST cost/acc:0.7174/52.6776%\n",
      "\n",
      "After \"Epoch\" 11, mini batch 10087 of 13755\n",
      "  TRAIN cost/acc:0.6524/60.7500%, TEST cost/acc:0.7115/54.4158%\n",
      " ... progress: In \"Epoch\" 11, mini batch 11000 of 13755\n",
      "  TRAIN cost/acc:0.6409/62.1600%, TEST cost/acc:0.7235/52.4793%\n",
      "\n",
      "After \"Epoch\" 12, mini batch 11004 of 13755\n",
      "  TRAIN cost/acc:0.6406/62.0400%, TEST cost/acc:0.7194/53.2522%\n",
      "\n",
      "After \"Epoch\" 13, mini batch 11921 of 13755\n",
      "  TRAIN cost/acc:0.6356/62.9300%, TEST cost/acc:0.7330/52.3606%\n",
      " ... progress: In \"Epoch\" 13, mini batch 12000 of 13755\n",
      "  TRAIN cost/acc:0.6042/66.3800%, TEST cost/acc:0.7304/52.5502%\n",
      "\n",
      "After \"Epoch\" 14, mini batch 12838 of 13755\n",
      "  TRAIN cost/acc:0.6184/64.7600%, TEST cost/acc:0.7355/51.9004%\n",
      " ... progress: In \"Epoch\" 14, mini batch 13000 of 13755\n",
      "  TRAIN cost/acc:0.6066/65.9800%, TEST cost/acc:0.7351/53.6459%\n",
      "\n",
      "After \"Epoch\" 15, mini batch 13755 of 13755\n",
      "  TRAIN cost/acc:0.6132/65.3700%, TEST cost/acc:0.7442/53.0409%\n",
      "\n",
      "FINISHED\n",
      "  TRAIN cost/acc:0.6132/65.3700%, TEST cost/acc:0.7442/53.0409%\n",
      "TRAINING took 173.34272835699812 seconds.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "# Initialize TensorFlow variables in Session\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def printStats(what):\n",
    "    check_features = 10000\n",
    "    train_cost, train_accuracy = sess.run([cost, accuracy], \n",
    "                                       feed_dict={X: trainFeatures[:check_features], labels: oh_trainLabels[:check_features]})\n",
    "    test_cost, test_accuracy = sess.run([cost, accuracy], \n",
    "                                     feed_dict={X: testFeatures, labels: oh_testLabels})\n",
    "    print(\"{}\\n  TRAIN cost/acc:{:.4f}/{:.4f}%, TEST cost/acc:{:.4f}/{:.4f}%\"\\\n",
    "          .format(what, train_cost, train_accuracy*100, test_cost, test_accuracy*100))\n",
    "\n",
    "printStats(\"BEFORE Training\")\n",
    "\n",
    "time_start = timer()\n",
    "epoch = 0\n",
    "for mini_batch in range(1, total_mini_batches+1):\n",
    "    batch_inputs, batch_labels = getTrainMiniBatch(mini_batch_size)\n",
    "    sess.run([train, accuracy], feed_dict={X: batch_inputs, \n",
    "                                           labels: batch_labels})\n",
    "    if (mini_batch % 1000 == 0):\n",
    "        # print(\".. progress: mini_batch/total_mini_batches: {}/{}\".format(mini_batch, total_mini_batches))\n",
    "        printStats(' ... progress: In \"Epoch\" {}, mini batch {} of {}'.format(epoch, mini_batch, total_mini_batches))\n",
    "        \n",
    "    if (mini_batch % mini_batches_per_epoch == 0):\n",
    "        epoch += 1\n",
    "        printStats('\\nAfter \"Epoch\" {}, mini batch {} of {}'.format(epoch, mini_batch, total_mini_batches))\n",
    "        \n",
    "training_time = timer()-time_start\n",
    "printStats(\"\\nFINISHED\")\n",
    "print(\"TRAINING took {} seconds.\".format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:endre-tf]",
   "language": "python",
   "name": "conda-env-endre-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
